# LLM-Based Post ASR Speech Emotion Recognition Challenge

## Overview

We are excited to introduce the inaugural **LLM-Based Post ASR Speech Emotion Recognition Challenge** (Part of the [SLT 2024 GenSEC Challenge](https://2024.ieeeslt.org/challenges/#1715507565729-916ec1d3-b60d)), the first challenge dedicated to exploring and advancing the capabilities of large language models (LLMs) in the field of speech emotion recognition. This challenge is designed to bring together enthusiasts, developers, and researchers to push the boundaries of emotional AI and speech analysis technology. Participants will have the opportunity to develop models that can accurately recognize and interpret emotions from spoken text (i.e., ASR Transcripts) using our provided dataset and scripts.

Note: This challenge is based on ASR-transcribed spoken text, **NOT** raw audio.

## Challenge

### Step 1: Request Training Data and Register for the Challenge

We use IEMOCAP dataset for this challenge. Please obtain its license first (if you already have it, please skip step 1).
- Submit a request to SAIL lab at USC use [this link](https://docs.google.com/forms/d/e/1FAIpQLScBecgI2K5bFTrXi_-05IYSSwOcqL5mX7dh57xcJV1m_NoznA/viewform).
- Register the challenge use [this link](https://docs.google.com/forms/d/102aDN45BpiDoUdS3ZqN63Q9oTFBcICPsvmo-5GFrU3U/viewform?ts=66321d62&edit_requested=true), attach the approved license or screenshot of the approval email from USC as proof. We will then email you the **curated data in JSON format that can be directly used in this script (i.e., you do not need to preprocess the dataset)**.

A data entry will look like this:

```json
 {
  "need_prediction": "no",
  "emotion": "fru",
  "id": "Ses01F_script01_1_F000",
  "speaker": "Ses01_F",
  "groundtruth": "What's he going to say?",
  "hubertlarge": "what's he ging to say",
  "w2v2100": "what's he gen to say",
  "w2v2960": "what's he going to say",
  "w2v2960large": "what's he going to say",
  "w2v2960largeself": "what's he going to say",
  "wavlmplus": "whats he gon to say",
  "whisperbase": "What's he gonna say?",
  "whisperlarge": "What's he gonna say?",
  "whispermedium": "What's he gonna say?",
  "whispersmall": "What's he gonna say?",
  "whispertiny": "What's he gonna say?"
 }
```

where 

- `need_prediction`: this key indicates whether this utterance should be included in the prediction procedure. "yes" denotes the utterances labeled with Big4 emotions, which are widely used for emotion recognition in IEMOCAP. "no" denotes all other utterances. Note that we have removed the utterances that have no human annotations.

- `emotion`: this key indicates the emotion label of the utterance.

- `id`: this key indicates the utterance ID, which is also the name of the audio file in IEMOCAP corpus. The ID is exactly the same as the raw ID in IEMOCAP.

- `speaker`: this key indicates the speaker of the utterance. Since there are two speakers in each session, there are ten speakers in total. It's important to note that the sixth character of the id DOES NOT represent the gender of the speaker, but rather the gender of the person currently wearing the motion capture device. Please use our provided speaker as the speaker ID.

- `groundtruth`: this key indicates the original human transcription provided by IEMOCAP.

The remaining ten keys indicate the ASR transcription generated by respective ASR model.

The unique aspect of the data is that it is already organized in the order of the conversation (each session is a conversation). **You can—and we highly encourage you to—use the conversation as context to predict the emotion of the target utterance.**

### Step 2: Build Your Model

We provide a [GPT-3.5-Turbo based method](https://colab.research.google.com/drive/11TIZBTBz1EiZA5DLtfEY5fqm0T1Xe6qp?usp=sharing) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/11TIZBTBz1EiZA5DLtfEY5fqm0T1Xe6qp?usp=sharing#scrollTo=tzE2qdmwcOvr) as the baseline. 

To encourage innovation, we do not place any restrictions on the methods used, as long as they are automated. For example, you are free to use:
- Any method, including LLM-based methods or conventional non-LLM based methods (e.g., BERT, LIWC, etc).
- Any large language model, including open-sourced ones (e.g., LLaMA, finetuned or original ones) or API-based ones (e.g., GPT-4).
- An arbitrary long context window.

### Step 3: Evaluation Phase

- Models will be evaluated with 4-class unweighted average accuracy.
- We will send out the evaluation data on **June 15th (tentative, subject to change)**.
- Participants must submit their predictions by **June 27th (tentative, subject to change)** to be eligible for the final assessment and potential awards.
- You can submit a 4-6 page paper to SLT2024 (by June 27th with update by July 7th), it will be included in the main SLT 2024 proceedings if accepted.

## Rules 

For fairness considerations. We have the following restrictions:

### Data

- You can use additional datasets to train your model, but they must NOT include IEMOCAP (except for the portion we provide to you). This is because we use part of IEMOCAP as our evaluation data. If additional datasets are used, you need to clearly mention the datasets used in the paper.
- For any datasets (including those we provide and your own), you can only use automatically transcribed text as model input, not raw audio or manually transcribed text.

### Method

- You cannot use raw audio data as input; the model must take text as input.
- You cannot manually transcribe the audio or annotate the emotion.
- You cannot use any ground-truth emotion label in the entire conversation as prior knowledge, we will not provide ground truth label for our evaluation set.
- You can use the conversation prior to the sentence requiring a prediction as context, but NOT the conversation that follows. I.e., it needs to be causal inference.
  
## What Can Be Explored?

We are excited about the interesting possibilities that can be explored through LLM-based emotion recognition. For example: 

1. **Context Length**: LLMs can consider the entire conversation as context, whereas conventional methods often treat each sentence separately. Does this broader contextual understanding enhance emotion recognition accuracy?
2. **Capturing Implicit Meanings**: LLMs are adept at understanding implicit meanings, such as sarcasm. How does this capability contribute to the nuanced detection of emotions in speech?
3. **Comparison of Large Language Models**: Which large language model performs best in this specific task of speech emotion recognition?
4. **Performance of Smaller Models**: Can smaller models outperform large language models in this challenge, perhaps through more efficient processing or specialized tuning?
5. **Challenging Emotions for LLMs**: Which emotion is particularly difficult for LLMs to predict accurately?

## Questions

If you have any questions regarding the challenge, please ask them in the GitHub issues section of our challenge repository.

## Organizers

- **Yuanchao Li**
  - Email: yuanchao.li@ed.ac.uk
- **Yuan Gong**
  - Email: yuangong@mit.edu

- **SLT24 GenSEC Challenge Organizer:** Huck Yang

## Reference

```
@article{busso2008iemocap,
  title={IEMOCAP: Interactive emotional dyadic motion capture database},
  author={Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},
  journal={Language resources and evaluation},
  volume={42},
  pages={335--359},
  year={2008},
  publisher={Springer}
}
```
```
@article{li2024speech,
  title={Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques},
  author={Li, Yuanchao and Bell, Peter and Lai, Catherine},
  journal={arXiv preprint arXiv:2406.08353},
  year={2024}
}
```
