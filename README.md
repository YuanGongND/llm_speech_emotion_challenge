# LLM-Based Post ASR Speech Emotion Challenge

## Overview

We are excited to introduce the inaugural **LLM-Based Post ASR Speech Emotion Challenge** (Part of the SLT 2024 GenSEC Challenge), the first challenge dedicated to exploring and advancing the capabilities of large language models (LLMs) in the field of speech emotion recognition. This challenge is designed to bring together enthusiasts, developers, and researchers to push the boundaries of emotional AI and speech analysis technology. Participants will have the opportunity to develop models that can accurately recognize and interpret emotions from spoken text (i.e., ASR Transcripts) using our provided dataset and scripts.

## Challenge

### Step 1: Request Training Data and Register for the Challenge

We use IEMOCAP dataset for this challenge. Please obtain its license first (if you already have it, please skip step 1).
- Submit a request to SAIL lab at USC use [this link](https://docs.google.com/forms/d/e/1FAIpQLScBecgI2K5bFTrXi_-05IYSSwOcqL5mX7dh57xcJV1m_NoznA/viewform).
- Register the challenge use [this link](https://docs.google.com/forms/d/102aDN45BpiDoUdS3ZqN63Q9oTFBcICPsvmo-5GFrU3U/viewform?ts=66321d62&edit_requested=true), attach the approved license or screenshot of the approval email from USC as proof. We will then email you the **curated data in JSON format that can be directly used in this script (i.e., you do not need to preprocess the dataset)**.

A data entry will look like this:

```json
 {
  "need_prediction": "no",
  "emotion": "fru",
  "id": "Ses01F_script01_1_F000",
  "speaker": "Ses01_F",
  "groundtruth": "What's he going to say?",
  "hubertlarge": "what's he ging to say",
  "w2v2100": "what's he gen to say",
  "w2v2960": "what's he going to say",
  "w2v2960large": "what's he going to say",
  "w2v2960largeself": "what's he going to say",
  "wavlmplus": "whats he gon to say",
  "whisperbase": "What's he gonna say?",
  "whisperlarge": "What's he gonna say?",
  "whispermedium": "What's he gonna say?",
  "whispersmall": "What's he gonna say?",
  "whispertiny": "What's he gonna say?"
 }
```

where 

- `need_prediction`: this key indicates whether this utterance should be included in the prediction procedure. "yes" denotes the utterances labeled with Big4 emotions, which are widely used for emotion recognition in IEMOCAP. "no" denotes all other utterances. Note that we have removed the utterances that have no human annotations.

- `emotion`: this key indicates the emotion label of the utterance.

- `id`: this key indicates the utterance ID, which is also the name of the audio file in IEMOCAP corpus. The ID is exactly the same as the raw ID in IEMOCAP.

- `speaker`: this key indicates the speaker of the utterance. Since there are two speakers in each session, there are ten speakers in total. It's important to note that the sixth character of the id DOES NOT represent the gender of the speaker, but rather the gender of the person currently wearing the motion capture device. Please use our provided speaker as the speaker ID.

- `groundtruth`: this key indicates the original human transcription provided by IEMOCAP.

The remaining ten keys indicate the ASR transcription generated by respective ASR model.

### Step 2: Build Your Model

We provide a [GPT-3.5-Turbo based method](https://colab.research.google.com/drive/11TIZBTBz1EiZA5DLtfEY5fqm0T1Xe6qp?usp=sharing) as the baseline. 

To encourage innovation, we do not place any restrictions on the methods used, as long as they are automated. For example, you are free to use:
- Any method, including LLM-based methods or conventional methods.
- Any large language model, including open-sourced ones (e.g., LLaMA, finetuned or original ones) or API-based ones (e.g., GPT-4).
- An arbitrary long context window.
  
However, for fairness considerations, the model must take our provided data (i.e., ASR transcripts) as input. You cannot:
- Use raw audio data as input.
- Use your own ASR model to transcribe the audio.

### Step 3: Evaluation Phase

- Models will be evaluated with 4-class unweighted average accuracy.
- We will send out the evaluation data on June 15th (tentative).
- Participants must submit their predictions by June 20th (tentative) to be eligible for the final assessment and potential awards.
- You can submit a 2-6 page paper to SLT2024 (by June 20th), it will be included in the main SLT 2024 proceedings once accepted.
  
## What Can Be Explored?

We are excited about the interesting possibilities that can be explored through LLM-based emotion recognition. For example: 

1. **Context Length**: LLMs can consider the entire conversation as context, whereas conventional methods often treat each sentence separately. Does this broader contextual understanding enhance emotion recognition accuracy?
2. **Capturing Implicit Meanings**: LLMs are adept at understanding implicit meanings, such as sarcasm. How does this capability contribute to the nuanced detection of emotions in speech?
3. **Comparison of Large Language Models**: Which large language model performs best in this specific task of speech emotion recognition?
4. **Performance of Smaller Models**: Can smaller models outperform large language models in this challenge, perhaps through more efficient processing or specialized tuning?

## Questions

If you have any questions regarding the challenge, please ask them in the GitHub issues section of our challenge repository.

## Organizers

### Co-organizers
- **Yuanchao Li**
  - Email: yuanchao.li@ed.ac.uk
- **Yuan Gong**
  - Email: yuangong@mit.edu

- **SLT24 GenSEC Challenge Organizer** Huck Yang
